import os
import math
import argparse

import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm

# å¯¼å…¥ä½ è‡ªå·±çš„æ¨¡å‹æ–‡ä»¶ã€æ•°æ®é›†æ–‡ä»¶å’Œæƒé‡åŠ è½½æ–‡ä»¶
from model import vit_base_patch16_224
from my_dataset import read_split_data, MyDataSet
from load_pretrained_weights import load_pretrained_weights


def train_one_epoch(model, optimizer, data_loader, device, epoch):
    model.train()
    loss_function = torch.nn.CrossEntropyLoss()
    accu_loss = torch.zeros(1).to(device)
    accu_num = torch.zeros(1).to(device)
    optimizer.zero_grad()

    sample_num = 0
    data_loader = tqdm(data_loader, desc=f"[è®­ç»ƒ Epoch {epoch}]")
    for step, data in enumerate(data_loader):
        images, labels = data
        sample_num += images.shape[0]

        pred = model(images.to(device))

        pred_classes = torch.max(pred, dim=1)[1]
        accu_num += torch.eq(pred_classes, labels.to(device)).sum()

        loss = loss_function(pred, labels.to(device))
        loss.backward()
        accu_loss += loss.detach()

        data_loader.set_postfix(loss=accu_loss.item() / (step + 1),
                                acc=accu_num.item() / sample_num)

        optimizer.step()
        optimizer.zero_grad()

    return accu_loss.item() / (step + 1), accu_num.item() / sample_num


@torch.no_grad()
def evaluate(model, data_loader, device, epoch):
    model.eval()
    loss_function = torch.nn.CrossEntropyLoss()
    accu_loss = torch.zeros(1).to(device)
    accu_num = torch.zeros(1).to(device)

    sample_num = 0
    data_loader = tqdm(data_loader, desc=f"[éªŒè¯ Epoch {epoch}]")
    for step, data in enumerate(data_loader):
        images, labels = data
        sample_num += images.shape[0]

        pred = model(images.to(device))
        pred_classes = torch.max(pred, dim=1)[1]
        accu_num += torch.eq(pred_classes, labels.to(device)).sum()

        loss = loss_function(pred, labels.to(device))
        accu_loss += loss

        data_loader.set_postfix(loss=accu_loss.item() / (step + 1),
                                acc=accu_num.item() / sample_num)

    return accu_loss.item() / (step + 1), accu_num.item() / sample_num


def main(args):
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}")

    if not os.path.exists("./weights"):
        os.makedirs("./weights")

    tb_writer = SummaryWriter()

    # --- ğŸŒŸ ä¿®å¤: ä»æ•°æ®è¯»å–å‡½æ•°ä¸­è·å–çœŸå®çš„ç±»åˆ«æ€»æ•° ---
    train_images_path, train_images_label, val_images_path, val_images_label, num_classes = read_split_data(
        args.data_path)

    # æ›´æ–°argsä¸­çš„ç±»åˆ«æ•°ï¼Œä»¥ä¿è¯åç»­ä»£ç ä½¿ç”¨æ­£ç¡®çš„æ•°å€¼
    args.num_classes = num_classes

    data_transform = {
        "train": transforms.Compose([transforms.RandomResizedCrop(224),
                                     transforms.RandomHorizontalFlip(),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),
        "val": transforms.Compose([transforms.Resize(256),
                                   transforms.CenterCrop(224),
                                   transforms.ToTensor(),
                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}

    train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label,
                              transform=data_transform["train"])
    val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform["val"])

    batch_size = args.batch_size
    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])
    print(f'æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ {nw} ä¸ª dataloader workers')

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,
                                               num_workers=nw, collate_fn=train_dataset.collate_fn)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True,
                                             num_workers=nw, collate_fn=val_dataset.collate_fn)

    # --- ğŸŒŸ ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„ç±»åˆ«æ•°åˆ›å»ºæ¨¡å‹ ---
    model = vit_base_patch16_224(num_classes=args.num_classes).to(device)

    if args.weights != "":
        assert os.path.exists(args.weights), f"æƒé‡æ–‡ä»¶: '{args.weights}' ä¸å­˜åœ¨."
        # å°†æ­£ç¡®çš„ç±»åˆ«æ•°ä¼ é€’ç»™æƒé‡åŠ è½½å‡½æ•°
        model = load_pretrained_weights(model, args.weights, args.num_classes)

    if args.freeze_layers:
        for name, para in model.named_parameters():
            if "head" not in name:
                para.requires_grad_(False)
            else:
                print(f"æ­£åœ¨è®­ç»ƒ {name}")

    pg = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.SGD(pg, lr=args.lr, momentum=0.9, weight_decay=5E-5)

    lf = lambda x: ((1 + math.cos(x * math.pi / args.epochs)) / 2) * (1 - args.lrf) + args.lrf
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)

    for epoch in range(args.epochs):
        train_loss, train_acc = train_one_epoch(model=model, optimizer=optimizer, data_loader=train_loader,
                                                device=device, epoch=epoch)
        scheduler.step()
        val_loss, val_acc = evaluate(model=model, data_loader=val_loader, device=device, epoch=epoch)

        tags = ["train_loss", "train_acc", "val_loss", "val_acc", "learning_rate"]
        tb_writer.add_scalar(tags[0], train_loss, epoch)
        tb_writer.add_scalar(tags[1], train_acc, epoch)
        tb_writer.add_scalar(tags[2], val_loss, epoch)
        tb_writer.add_scalar(tags[3], val_acc, epoch)
        tb_writer.add_scalar(tags[4], optimizer.param_groups[0]["lr"], epoch)

        torch.save(model.state_dict(), f"./weights/model-{epoch}.pth")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # æˆ‘ä»¬å¯ä»¥ä¿ç•™è¿™ä¸ªé»˜è®¤å€¼ï¼Œä½†ä»£ç ç°åœ¨ä¼šè‡ªåŠ¨è¦†ç›–å®ƒ
    parser.add_argument('--num_classes', type=int, default=5)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=8)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--lrf', type=float, default=0.01)

    parser.add_argument('--data-path', type=str, default="../data/flower_photos")
    parser.add_argument('--weights', type=str, default='./pretrained_weights/vit_base_patch16_224_in21k.pth',
                        help='initial weights path')

    parser.add_argument('--freeze-layers', type=bool, default=True)
    parser.add_argument('--device', default='cuda:0', help='device id (i.e. 0 or 0,1 or cpu)')

    opt = parser.parse_args()
    main(opt)
