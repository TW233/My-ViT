import os
import json
import random
from PIL import Image
import torch
from torch.utils.data import Dataset
from tqdm import tqdm


def read_split_data(root: str, val_rate: float = 0.2):
    """
    åŠŸèƒ½: åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶è¿”å›å›¾ç‰‡è·¯å¾„ã€æ ‡ç­¾ä»¥åŠå‘ç°çš„ç±»åˆ«æ€»æ•°ã€‚
    å‚æ•°:
        root: æ•°æ®é›†æ ¹ç›®å½•ï¼Œç›®å½•ä¸‹æ¯ä¸ªå­æ–‡ä»¶å¤¹ä»£è¡¨ä¸€ä¸ªç±»åˆ«ã€‚
        val_rate: éªŒè¯é›†æ‰€å çš„æ¯”ä¾‹ã€‚
    è¿”å›:
        è®­ç»ƒé›†å›¾ç‰‡è·¯å¾„åˆ—è¡¨, è®­ç»ƒé›†æ ‡ç­¾åˆ—è¡¨, éªŒè¯é›†å›¾ç‰‡è·¯å¾„åˆ—è¡¨, éªŒè¯é›†æ ‡ç­¾åˆ—è¡¨, ç±»åˆ«æ€»æ•°
    """
    random.seed(0)
    assert os.path.exists(root), f"dataset root: {root} does not exist."

    # è‡ªåŠ¨å¯»æ‰¾æ•°æ®æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]
    flower_class.sort()

    # --- ğŸŒŸ ä¿®å¤: åŠ¨æ€åœ°ä»æ–‡ä»¶å¤¹æ•°é‡è·å–ç±»åˆ«æ€»æ•° ---
    num_classes = len(flower_class)

    # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
    class_indices = dict((k, v) for v, k in enumerate(flower_class))
    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)
    with open('class_indices.json', 'w') as json_file:
        json_file.write(json_str)

    train_images_path = []
    train_images_label = []
    val_images_path = []
    val_images_label = []
    every_class_num = []
    supported = [".jpg", ".JPG", ".png", ".PNG"]

    for cla in flower_class:
        cla_path = os.path.join(root, cla)
        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)
                  if os.path.splitext(i)[-1] in supported]
        images.sort()

        image_class = class_indices[cla]
        every_class_num.append(len(images))

        val_path = random.sample(images, k=int(len(images) * val_rate))

        for img_path in images:
            if img_path in val_path:
                val_images_path.append(img_path)
                val_images_label.append(image_class)
            else:
                train_images_path.append(img_path)
                train_images_label.append(image_class)

    print(f"åœ¨æ•°æ®é›†ä¸­å…±å‘ç° {sum(every_class_num)} å¼ å›¾ç‰‡ã€‚")
    print(f"å…±å‘ç° {num_classes} ä¸ªç±»åˆ«ã€‚")  # æ‰“å°å‡ºå‘ç°çš„ç±»åˆ«æ•°
    print(f"{len(train_images_path)} å¼ å›¾ç‰‡ç”¨äºè®­ç»ƒã€‚")
    print(f"{len(val_images_path)} å¼ å›¾ç‰‡ç”¨äºéªŒè¯ã€‚")

    # --- ğŸŒŸ ä¿®å¤: è¿”å›å‘ç°çš„ç±»åˆ«æ€»æ•° ---
    return train_images_path, train_images_label, val_images_path, val_images_label, num_classes


class MyDataSet(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»"""

    def __init__(self, images_path: list, images_class: list, transform=None):
        self.images_path = images_path
        self.images_class = images_class
        self.transform = transform

    def __len__(self):
        return len(self.images_path)

    def __getitem__(self, item):
        img = Image.open(self.images_path[item])
        if img.mode != 'RGB':
            raise ValueError(f"image: {self.images_path[item]} isn't RGB mode.")
        label = self.images_class[item]

        if self.transform is not None:
            img = self.transform(img)

        return img, label

    @staticmethod
    def collate_fn(batch):
        images, labels = tuple(zip(*batch))
        images = torch.stack(images, dim=0)
        labels = torch.as_tensor(labels)
        return images, labels
